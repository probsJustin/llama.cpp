version: '3.8'

services:
  # Jaeger all-in-one for tracing visualization
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # UI
      - "14250:14250"  # gRPC collector
      - "14268:14268"  # HTTP collector
      - "14269:14269"  # Health check
      - "4317:4317"    # OTLP gRPC receiver
      - "4318:4318"    # OTLP HTTP receiver
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - llama-observability

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    command: ["--config=/etc/otel-collector-config.yaml"]
    ports:
      - "8888:8888"   # Metrics endpoint for the collector itself
      - "8889:8889"   # Prometheus endpoint
      - "13133:13133" # Health check
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "55679:55679" # zpages extension
    depends_on:
      - jaeger
    networks:
      - llama-observability

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - llama-observability
    depends_on:
      - otel-collector

  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SECURITY_ADMIN_USER=admin
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - grafana-data:/var/lib/grafana
    networks:
      - llama-observability
    depends_on:
      - prometheus

  # llama.cpp server with OpenTelemetry instrumentation
  llama-cpp:
    build:
      context: ../..
      dockerfile: ./observability/dockerfiles/Dockerfile.llama-cpp
    ports:
      - "8080:8080"   # Server API endpoint
    volumes:
      - ../../models:/app/models
    environment:
      - OTEL_SERVICE_NAME=llama-cpp
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_RESOURCE_ATTRIBUTES=service.name=llama-cpp,service.version=1.0.0
      - OTEL_TRACES_SAMPLER=always_on
      - OTEL_METRICS_EXPORTER=otlp
      - OTEL_LOGS_EXPORTER=otlp
    networks:
      - llama-observability
    depends_on:
      - otel-collector
    # Uncomment to start the server automatically
    # command: ["./server", "-m", "/app/models/7B/ggml-model-q4_0.gguf", "--host", "0.0.0.0", "--port", "8080"]

networks:
  llama-observability:
    driver: bridge

volumes:
  grafana-data: